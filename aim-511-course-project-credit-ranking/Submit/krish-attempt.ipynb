{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "1. **Load the Dataset**  \n",
    "   - The training dataset is loaded from `train.csv` and stored in `train_data`.\n",
    "   - The test dataset is also loaded from `test.csv` and stored in `test_data`.\n",
    "   - The `ID` column is separately loaded from the test dataset to create `test_ids`, which will likely be used for referencing test predictions later.\n",
    "\n",
    "2. **Analyze the Dataset**  \n",
    "   - Several columns that are not essential for modeling, including `ID`, `Customer_ID`, `Month`, `Name`, `Profession`, `Number`, and `Loan_Type`, are removed from the `train_data` DataFrame. This simplifies the dataset by retaining only the most relevant features for the predictive model.\n",
    "   - The `train_data.head()` function displays the first few rows of the modified training dataset, giving a quick preview of the remaining columns.\n",
    "\n",
    "3. **Categorical Columns**  \n",
    "   - Certain columns, such as `Loan_Type`, `Credit_Mix`, `Payment_of_Min_Amount`, `Payment_Behaviour`, and `Credit_Score`, contain categorical data.\n",
    "   - The `Credit_Score` column, which indicates the creditworthiness of customers, is identified as the target variable for the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_9972\\3789247019.py:2: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(\"train.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Age Income_Annual  Base_Salary_PerMonth  Total_Bank_Accounts  \\\n",
      "0  51     101583.48                   NaN                    5   \n",
      "1  23     101926.95             8635.9125                    4   \n",
      "2  49     158871.12                   NaN                    0   \n",
      "3  40      60379.28                   NaN                    5   \n",
      "4  17      50050.83             4085.9025                    9   \n",
      "\n",
      "   Total_Credit_Cards  Rate_Of_Interest Total_Current_Loans  \\\n",
      "0                   7                10                  4_   \n",
      "1                   4                 9                   1   \n",
      "2                   4                 8                   1   \n",
      "3                   6                18                   3   \n",
      "4                  10                20                   5   \n",
      "\n",
      "   Delay_from_due_date Total_Delayed_Payments Credit_Limit  ...  Credit_Mix  \\\n",
      "0                    8                      8         2.89  ...    Standard   \n",
      "1                   13                      9        10.26  ...           _   \n",
      "2                    8                      2         1.17  ...        Good   \n",
      "3                   15                     12         6.83  ...    Standard   \n",
      "4                   28                    NaN        15.45  ...         Bad   \n",
      "\n",
      "  Current_Debt_Outstanding Ratio_Credit_Utilization     Credit_History_Age  \\\n",
      "0                    50.93                34.462154  24 Years and 1 Months   \n",
      "1                     1058                39.693812  20 Years and 5 Months   \n",
      "2                   576.48                39.367225  19 Years and 0 Months   \n",
      "3                   725.39                29.061701  17 Years and 1 Months   \n",
      "4                   3419.1                30.386321   4 Years and 6 Months   \n",
      "\n",
      "  Payment_of_Min_Amount Per_Month_EMI  Monthly_Investment  \\\n",
      "0                    No    190.811017   630.0157894388726   \n",
      "1                    No     70.587681   662.8039273360225   \n",
      "2                    No     86.905860   746.8059854204569   \n",
      "3                    NM     90.906385  166.41865803064803   \n",
      "4                   Yes    190.445060  56.789441169542684   \n",
      "\n",
      "                  Payment_Behaviour     Monthly_Balance Credit_Score  \n",
      "0    Low_spent_Large_value_payments   314.0021934422197     Standard  \n",
      "1   Low_spent_Medium_value_payments   410.1996419555151     Standard  \n",
      "2   Low_spent_Medium_value_payments   742.5141542054829     Standard  \n",
      "3  High_spent_Medium_value_payments  473.13562343490486     Standard  \n",
      "4   High_spent_Large_value_payments   401.3557486786916         Poor  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_ids = pd.read_csv(\"test.csv\")[\"ID\"]\n",
    "# Analyze your dataset\n",
    "# ID, Customer_ID, Month, Name, Age, Profession, Number are columns to be dropped\n",
    "train_data = train_data.drop(columns=[\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"Profession\", \"Number\", \"Loan_Type\"])\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# Loan_Type, Credit_Mix, Payment_of_Min_Amount, Payment_Behaviour , and Credit_Score are categorical columns\n",
    "# Credit_Score is the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "1. **Fill Missing Values with Median**  \n",
    "   - The `fillna()` function is used to replace any missing values in the `train_data` DataFrame with the median of each column.\n",
    "   - The `train_data.median(numeric_only=True)` calculation returns the median values of the numerical columns in `train_data`. This approach minimizes the impact of outliers, making it a robust choice for imputation.\n",
    "   - `inplace=True` modifies `train_data` directly, updating it without needing to reassign it to a new variable.\n",
    "\n",
    "2. **Preview Data After Imputation**  \n",
    "   - The `print(train_data.head())` statement displays the first few rows of `train_data` to verify that missing values have been replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Age Income_Annual  Base_Salary_PerMonth  Total_Bank_Accounts  \\\n",
      "0  51     101583.48           3086.683333                    5   \n",
      "1  23     101926.95           8635.912500                    4   \n",
      "2  49     158871.12           3086.683333                    0   \n",
      "3  40      60379.28           3086.683333                    5   \n",
      "4  17      50050.83           4085.902500                    9   \n",
      "\n",
      "   Total_Credit_Cards  Rate_Of_Interest Total_Current_Loans  \\\n",
      "0                   7                10                  4_   \n",
      "1                   4                 9                   1   \n",
      "2                   4                 8                   1   \n",
      "3                   6                18                   3   \n",
      "4                  10                20                   5   \n",
      "\n",
      "   Delay_from_due_date Total_Delayed_Payments Credit_Limit  ...  Credit_Mix  \\\n",
      "0                    8                      8         2.89  ...    Standard   \n",
      "1                   13                      9        10.26  ...           _   \n",
      "2                    8                      2         1.17  ...        Good   \n",
      "3                   15                     12         6.83  ...    Standard   \n",
      "4                   28                    NaN        15.45  ...         Bad   \n",
      "\n",
      "  Current_Debt_Outstanding Ratio_Credit_Utilization     Credit_History_Age  \\\n",
      "0                    50.93                34.462154  24 Years and 1 Months   \n",
      "1                     1058                39.693812  20 Years and 5 Months   \n",
      "2                   576.48                39.367225  19 Years and 0 Months   \n",
      "3                   725.39                29.061701  17 Years and 1 Months   \n",
      "4                   3419.1                30.386321   4 Years and 6 Months   \n",
      "\n",
      "  Payment_of_Min_Amount Per_Month_EMI  Monthly_Investment  \\\n",
      "0                    No    190.811017   630.0157894388726   \n",
      "1                    No     70.587681   662.8039273360225   \n",
      "2                    No     86.905860   746.8059854204569   \n",
      "3                    NM     90.906385  166.41865803064803   \n",
      "4                   Yes    190.445060  56.789441169542684   \n",
      "\n",
      "                  Payment_Behaviour     Monthly_Balance Credit_Score  \n",
      "0    Low_spent_Large_value_payments   314.0021934422197     Standard  \n",
      "1   Low_spent_Medium_value_payments   410.1996419555151     Standard  \n",
      "2   Low_spent_Medium_value_payments   742.5141542054829     Standard  \n",
      "3  High_spent_Medium_value_payments  473.13562343490486     Standard  \n",
      "4   High_spent_Large_value_payments   401.3557486786916         Poor  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with median in train data\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Type Conversion\n",
    "\n",
    "1. **Removing Underscores and Converting Columns to Numeric**  \n",
    "   - A `for` loop iterates over each column in the specified list of columns that are expected to contain numeric data but may have underscore characters (e.g., `\"_\"`) in the values, which can interfere with numeric conversion.\n",
    "   - For each column:\n",
    "     - In `train_data`, the column values are first converted to strings and any underscores are removed using `str.replace(\"_\", \"\", regex=False)`.\n",
    "     - After removing underscores, `pd.to_numeric()` converts the cleaned string values to numeric values.\n",
    "     - The `errors=\"coerce\"` parameter ensures that any non-numeric values that remain after removing underscores are replaced with `NaN`, allowing for consistent data handling.\n",
    "   - The same cleaning and conversion process is applied to each corresponding column in `test_data`.\n",
    "\n",
    "2. **Preview of Additional Columns to Consider**  \n",
    "   - The comment block below the code lists additional columns that may also require cleaning or conversion, such as `Income_Annual`, `Base_Salary_PerMonth`, and `Current_Debt_Outstanding`. These columns may need further processing to ensure they are in the correct format for analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income_Annual</th>\n",
       "      <th>Base_Salary_PerMonth</th>\n",
       "      <th>Total_Bank_Accounts</th>\n",
       "      <th>Total_Credit_Cards</th>\n",
       "      <th>Rate_Of_Interest</th>\n",
       "      <th>Total_Current_Loans</th>\n",
       "      <th>Delay_from_due_date</th>\n",
       "      <th>Total_Delayed_Payments</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>...</th>\n",
       "      <th>Credit_Mix</th>\n",
       "      <th>Current_Debt_Outstanding</th>\n",
       "      <th>Ratio_Credit_Utilization</th>\n",
       "      <th>Credit_History_Age</th>\n",
       "      <th>Payment_of_Min_Amount</th>\n",
       "      <th>Per_Month_EMI</th>\n",
       "      <th>Monthly_Investment</th>\n",
       "      <th>Payment_Behaviour</th>\n",
       "      <th>Monthly_Balance</th>\n",
       "      <th>Credit_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>101583.480</td>\n",
       "      <td>3086.683333</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.89</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>50.93</td>\n",
       "      <td>34.462154</td>\n",
       "      <td>24 Years and 1 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>190.811017</td>\n",
       "      <td>630.0157894388726</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>314.0021934422197</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>101926.950</td>\n",
       "      <td>8635.912500</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10.26</td>\n",
       "      <td>...</td>\n",
       "      <td>_</td>\n",
       "      <td>1058.00</td>\n",
       "      <td>39.693812</td>\n",
       "      <td>20 Years and 5 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>70.587681</td>\n",
       "      <td>662.8039273360225</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>410.1996419555151</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>158871.120</td>\n",
       "      <td>3086.683333</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>576.48</td>\n",
       "      <td>39.367225</td>\n",
       "      <td>19 Years and 0 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>86.905860</td>\n",
       "      <td>746.8059854204569</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>742.5141542054829</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>60379.280</td>\n",
       "      <td>3086.683333</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>6.83</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>725.39</td>\n",
       "      <td>29.061701</td>\n",
       "      <td>17 Years and 1 Months</td>\n",
       "      <td>NM</td>\n",
       "      <td>90.906385</td>\n",
       "      <td>166.41865803064803</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>473.13562343490486</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>50050.830</td>\n",
       "      <td>4085.902500</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.45</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>3419.10</td>\n",
       "      <td>30.386321</td>\n",
       "      <td>4 Years and 6 Months</td>\n",
       "      <td>Yes</td>\n",
       "      <td>190.445060</td>\n",
       "      <td>56.789441169542684</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>401.3557486786916</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>55</td>\n",
       "      <td>114597.040</td>\n",
       "      <td>3086.683333</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>10.54</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>926.18</td>\n",
       "      <td>26.436313</td>\n",
       "      <td>31 Years and 9 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>225.923762</td>\n",
       "      <td>327.61966834569836</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>633.131903</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>28</td>\n",
       "      <td>8227.855</td>\n",
       "      <td>656.654583</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>-100</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>17.60</td>\n",
       "      <td>...</td>\n",
       "      <td>_</td>\n",
       "      <td>2695.38</td>\n",
       "      <td>24.127401</td>\n",
       "      <td>11 Years and 8 Months</td>\n",
       "      <td>NM</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>49.54415830254037</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>268.108435</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>46</td>\n",
       "      <td>35032.660</td>\n",
       "      <td>2853.388333</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>2.52</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>1789.00</td>\n",
       "      <td>25.086176</td>\n",
       "      <td>11 Years and 5 Months</td>\n",
       "      <td>Yes</td>\n",
       "      <td>150.500097</td>\n",
       "      <td>106.73567925309915</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>318.103057</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>42</td>\n",
       "      <td>129680.280</td>\n",
       "      <td>10643.690000</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>16.65</td>\n",
       "      <td>...</td>\n",
       "      <td>_</td>\n",
       "      <td>240.27</td>\n",
       "      <td>33.944094</td>\n",
       "      <td>20 Years and 5 Months</td>\n",
       "      <td>NM</td>\n",
       "      <td>114.165609</td>\n",
       "      <td>567.1798727916067</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>643.023518</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>53</td>\n",
       "      <td>112853.220</td>\n",
       "      <td>9552.435000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.06</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>732.33</td>\n",
       "      <td>32.195044</td>\n",
       "      <td>17 Years and 2 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>127.072288</td>\n",
       "      <td>219.89003507274796</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>858.281177</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Income_Annual  Base_Salary_PerMonth  Total_Bank_Accounts  \\\n",
       "0       51     101583.480           3086.683333                    5   \n",
       "1       23     101926.950           8635.912500                    4   \n",
       "2       49     158871.120           3086.683333                    0   \n",
       "3       40      60379.280           3086.683333                    5   \n",
       "4       17      50050.830           4085.902500                    9   \n",
       "...    ...            ...                   ...                  ...   \n",
       "79995   55     114597.040           3086.683333                    7   \n",
       "79996   28       8227.855            656.654583                    6   \n",
       "79997   46      35032.660           2853.388333                   10   \n",
       "79998   42     129680.280          10643.690000                    8   \n",
       "79999   53     112853.220           9552.435000                    2   \n",
       "\n",
       "       Total_Credit_Cards  Rate_Of_Interest  Total_Current_Loans  \\\n",
       "0                       7                10                    4   \n",
       "1                       4                 9                    1   \n",
       "2                       4                 8                    1   \n",
       "3                       6                18                    3   \n",
       "4                      10                20                    5   \n",
       "...                   ...               ...                  ...   \n",
       "79995                   6                 4                    4   \n",
       "79996                   8                31                 -100   \n",
       "79997                   6                33                    7   \n",
       "79998                   3                 5                    2   \n",
       "79999                   4                 4                    2   \n",
       "\n",
       "       Delay_from_due_date Total_Delayed_Payments  Credit_Limit  ...  \\\n",
       "0                        8                      8          2.89  ...   \n",
       "1                       13                      9         10.26  ...   \n",
       "2                        8                      2          1.17  ...   \n",
       "3                       15                     12          6.83  ...   \n",
       "4                       28                    NaN         15.45  ...   \n",
       "...                    ...                    ...           ...  ...   \n",
       "79995                   32                      8         10.54  ...   \n",
       "79996                   61                     20         17.60  ...   \n",
       "79997                   48                     18          2.52  ...   \n",
       "79998                   27                     18         16.65  ...   \n",
       "79999                    3                    NaN          8.06  ...   \n",
       "\n",
       "       Credit_Mix Current_Debt_Outstanding  Ratio_Credit_Utilization  \\\n",
       "0        Standard                    50.93                 34.462154   \n",
       "1               _                  1058.00                 39.693812   \n",
       "2            Good                   576.48                 39.367225   \n",
       "3        Standard                   725.39                 29.061701   \n",
       "4             Bad                  3419.10                 30.386321   \n",
       "...           ...                      ...                       ...   \n",
       "79995        Good                   926.18                 26.436313   \n",
       "79996           _                  2695.38                 24.127401   \n",
       "79997         Bad                  1789.00                 25.086176   \n",
       "79998           _                   240.27                 33.944094   \n",
       "79999        Good                   732.33                 32.195044   \n",
       "\n",
       "          Credit_History_Age Payment_of_Min_Amount Per_Month_EMI  \\\n",
       "0      24 Years and 1 Months                    No    190.811017   \n",
       "1      20 Years and 5 Months                    No     70.587681   \n",
       "2      19 Years and 0 Months                    No     86.905860   \n",
       "3      17 Years and 1 Months                    NM     90.906385   \n",
       "4       4 Years and 6 Months                   Yes    190.445060   \n",
       "...                      ...                   ...           ...   \n",
       "79995  31 Years and 9 Months                    No    225.923762   \n",
       "79996  11 Years and 8 Months                    NM   7352.000000   \n",
       "79997  11 Years and 5 Months                   Yes    150.500097   \n",
       "79998  20 Years and 5 Months                    NM    114.165609   \n",
       "79999  17 Years and 2 Months                    No    127.072288   \n",
       "\n",
       "       Monthly_Investment                 Payment_Behaviour  \\\n",
       "0       630.0157894388726    Low_spent_Large_value_payments   \n",
       "1       662.8039273360225   Low_spent_Medium_value_payments   \n",
       "2       746.8059854204569   Low_spent_Medium_value_payments   \n",
       "3      166.41865803064803  High_spent_Medium_value_payments   \n",
       "4      56.789441169542684   High_spent_Large_value_payments   \n",
       "...                   ...                               ...   \n",
       "79995  327.61966834569836  High_spent_Medium_value_payments   \n",
       "79996   49.54415830254037   Low_spent_Medium_value_payments   \n",
       "79997  106.73567925309915    Low_spent_Small_value_payments   \n",
       "79998   567.1798727916067   High_spent_Small_value_payments   \n",
       "79999  219.89003507274796  High_spent_Medium_value_payments   \n",
       "\n",
       "          Monthly_Balance Credit_Score  \n",
       "0       314.0021934422197     Standard  \n",
       "1       410.1996419555151     Standard  \n",
       "2       742.5141542054829     Standard  \n",
       "3      473.13562343490486     Standard  \n",
       "4       401.3557486786916         Poor  \n",
       "...                   ...          ...  \n",
       "79995          633.131903         Poor  \n",
       "79996          268.108435         Poor  \n",
       "79997          318.103057         Poor  \n",
       "79998          643.023518     Standard  \n",
       "79999          858.281177     Standard  \n",
       "\n",
       "[80000 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert relevant columns to numeric after removing any underscores.This was done as I saw a value in Total_Current_Loans with an underscore.\n",
    "for col in [\"Total_Current_Loans\", \"Current_Debt_Outstanding\", \"Income_Annual\", \"Credit_Limit\", \"Age\", \"Total_Credit_Cards\", \"Total_Bank_Accounts\", \"Delay_from_due_date\"]:\n",
    "    train_data[col] = pd.to_numeric(\n",
    "        train_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "    test_data[col] = pd.to_numeric(\n",
    "        test_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "train_data\n",
    "# Income_Annual, Base_Salary_PerMonth,Current_Debt_Outstanding,Ratio_Credit_Utilization, Per_Month_EMI, Monthly_Investment\n",
    "\n",
    "# 'Total_Delayed_Payments', 'Credit_Mix',\n",
    "#        'Credit_History_Age', 'Payment_of_Min_Amount', 'Monthly_Investment',\n",
    "#        'Payment_Behaviour', 'Monthly_Balance'],\n",
    "#       dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting `Credit_History_Age` to Numeric Format\n",
    "\n",
    "1. **Extracting Numerical Part from `Credit_History_Age`**  \n",
    "   - The `Credit_History_Age` column contains values in a text format such as `\"1 Years and 2 Months\"`. To use this data numerically, we extract the numeric portion representing the years.\n",
    "   - `str.extract(\"(\\d+)\")` uses a regular expression to capture only the numeric part (the number of years) from each entry, ignoring the rest of the text. `(\\d+)` matches one or more digits.\n",
    "\n",
    "2. **Converting Extracted Data to Float**  \n",
    "   - After extraction, `.astype(float)` converts the resulting values to a float data type, ensuring the column is in numeric format and can be used in calculations or models.\n",
    "   - This process is applied to both `train_data` and `test_data`, ensuring consistency across both datasets.\n",
    "\n",
    "3. **Previewing the Converted Column**  \n",
    "   - The final line displays the transformed `Credit_History_Age` column from `train_data`, allowing verification of the conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        24.0\n",
       "1        20.0\n",
       "2        19.0\n",
       "3        17.0\n",
       "4         4.0\n",
       "         ... \n",
       "79995    31.0\n",
       "79996    11.0\n",
       "79997    11.0\n",
       "79998    20.0\n",
       "79999    17.0\n",
       "Name: Credit_History_Age, Length: 80000, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert credit_history_age to numeric\n",
    "# Format : 1 Years and 2 Months\n",
    "train_data[\"Credit_History_Age\"] = train_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "test_data[\"Credit_History_Age\"] = test_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "train_data[\"Credit_History_Age\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Infinity Values and Encoding the Target Variable\n",
    "\n",
    "1. **Replacing Infinity Values**  \n",
    "   - `train_data.replace([np.inf, -np.inf], np.nan, inplace=True)` searches for any infinity (`np.inf`) or negative infinity (`-np.inf`) values in `train_data` and replaces them with `NaN`. This ensures that infinity values, which may disrupt model training, are handled appropriately.\n",
    "\n",
    "2. **Filling Missing Values with Median**  \n",
    "   - After replacing infinity values with `NaN`, `train_data.fillna(train_data.median(numeric_only=True), inplace=True)` fills any remaining `NaN` values (including those from the previous step) with the median of each numeric column. This step ensures that the dataset remains complete and without missing values.\n",
    "\n",
    "3. **Label Encoding the Target Variable**  \n",
    "   - `LabelEncoder()` is used to convert the target column `Credit_Score` into numerical format. This is especially useful if `Credit_Score` is categorical (e.g., \"Good\", \"Average\", \"Bad\").\n",
    "   - `label_encoder.fit_transform(train_data[\"Credit_Score\"])` transforms the categories in `Credit_Score` to numerical labels, which is necessary for most machine learning models.\n",
    "   - The modified `Credit_Score` column in `train_data` now contains integer values representing each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity values in train data\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Label encode the target variable in train data\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"Credit_Score\"] = label_encoder.fit_transform(train_data[\"Credit_Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training Features and Labels\n",
    "\n",
    "1. **Separating Features (X_train)**  \n",
    "   - `X_train` is created by dropping the target column, `Credit_Score`, from `train_data`. This DataFrame now contains only the feature columns that will be used as inputs for the model.\n",
    "\n",
    "2. **Separating Labels (y_train)**  \n",
    "   - `y_train` is created by selecting only the `Credit_Score` column from `train_data`. This Series represents the target variable, containing the labels that the model will learn to predict.\n",
    "\n",
    "Together, `X_train` and `y_train` form the feature set and label set for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training features and labels\n",
    "X_train = train_data.drop(columns=\"Credit_Score\")\n",
    "y_train = train_data[\"Credit_Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering in `train_data`\n",
    "\n",
    "1. **Creating `Debt_Income_Ratio`**  \n",
    "   - This new feature calculates the ratio of `Current_Debt_Outstanding` to `Income_Annual`. It provides insight into the debt burden relative to income, which can be an important indicator of credit risk.\n",
    "\n",
    "2. **Creating `Income_Credit_Limit_Ratio`**  \n",
    "   - This feature is the ratio of `Income_Annual` to `Credit_Limit`. It helps assess the income capacity against the available credit limit, which can indicate how likely a customer might be to max out their credit.\n",
    "\n",
    "3. **Creating `Debt_Credit_Limit_Ratio`**  \n",
    "   - This feature is calculated as the ratio of `Current_Debt_Outstanding` to `Credit_Limit`. It measures the proportion of debt relative to the available credit limit, which can indicate the extent to which a customer relies on credit.\n",
    "\n",
    "These new features enhance the dataset by providing additional, potentially predictive variables that may improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering in train data\n",
    "train_data[\"Debt_Income_Ratio\"] = (\n",
    "    train_data[\"Current_Debt_Outstanding\"] / train_data[\"Income_Annual\"]\n",
    ")\n",
    "train_data[\"Income_Credit_Limit_Ratio\"] = (\n",
    "    train_data[\"Income_Annual\"] / train_data[\"Credit_Limit\"]\n",
    ")\n",
    "train_data[\"Debt_Credit_Limit_Ratio\"] = (\n",
    "    train_data[\"Current_Debt_Outstanding\"] / train_data[\"Credit_Limit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Converting Monthly Balance and Monthly Investment Columns\n",
    "\n",
    "1. **Inspecting Data Columns**  \n",
    "   - The code includes commented-out lines to print the `Monthly_Balance` and `Monthly_Investment` columns and identify object-type columns in `X_train`. These checks can help understand the initial data format and identify any issues before conversion.\n",
    "\n",
    "2. **Converting `Monthly_Balance` to Numeric**  \n",
    "   - `X_train[\"Monthly_Balance\"]` is converted to numeric by first converting it to a string, removing any underscores (`_`) that may be present, and then using `pd.to_numeric` for conversion.\n",
    "   - `errors=\"coerce\"` replaces any non-numeric values with `NaN`, ensuring the column is consistently numeric.\n",
    "\n",
    "3. **Converting `Monthly_Investment` to Numeric**  \n",
    "   - Similarly, `X_train[\"Monthly_Investment\"]` is converted by removing underscores and coercing any invalid values to `NaN`.\n",
    "\n",
    "4. **Filling Missing Values with Median**  \n",
    "   - Any `NaN` values in `X_train` resulting from the conversion process are filled with the median values of the numeric columns to maintain data completeness.\n",
    "\n",
    "5. **Applying the Same Cleaning Steps to `test_data`**  \n",
    "   - The `test_data` DataFrame undergoes the same cleaning and conversion process for `Monthly_Balance` and `Monthly_Investment`, ensuring both datasets are treated consistently.\n",
    "\n",
    "6. **Printing the Converted Columns**  \n",
    "   - Finally, the code prints `Monthly_Investment` and `Monthly_Balance` from `X_train` to verify the transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        630.015789\n",
      "1        662.803927\n",
      "2        746.805985\n",
      "3        166.418658\n",
      "4         56.789441\n",
      "            ...    \n",
      "79995    327.619668\n",
      "79996     49.544158\n",
      "79997    106.735679\n",
      "79998    567.179873\n",
      "79999    219.890035\n",
      "Name: Monthly_Investment, Length: 80000, dtype: float64 0        314.002193\n",
      "1        410.199642\n",
      "2        742.514154\n",
      "3        473.135623\n",
      "4        401.355749\n",
      "            ...    \n",
      "79995    633.131903\n",
      "79996    268.108435\n",
      "79997    318.103057\n",
      "79998    643.023518\n",
      "79999    858.281177\n",
      "Name: Monthly_Balance, Length: 80000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print Monthly_Balance column\n",
    "# print(X_train[\"Monthly_Investment\"])\n",
    "# print(X_train.select_dtypes(include=[\"object\"]).columns)\n",
    "# Replace all str values in Monthly_Balance, Monthly_Investment\n",
    "\n",
    "X_train[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "    X_train[\"Monthly_Balance\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "X_train[\"Monthly_Investment\"] = pd.to_numeric(\n",
    "    X_train[\"Monthly_Investment\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Fill missing values with median for Monthly_Balance and Monthly_Investment\n",
    "X_train.fillna(X_train.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Same for test data\n",
    "test_data[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "    test_data[\"Monthly_Balance\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "test_data[\"Monthly_Investment\"] = pd.to_numeric(\n",
    "    test_data[\"Monthly_Investment\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "test_data.fillna(test_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# print Monthly_Balance column\n",
    "print(X_train[\"Monthly_Investment\"], X_train[\"Monthly_Balance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Training\n",
    "\n",
    "1. **Replacing Placeholder Values with `NaN`**  \n",
    "   - `X_train.replace(\"-\", np.nan, inplace=True)` and `X_train.replace(\"NM\", np.nan, inplace=True)` replace any instances of `\"-\"` and `\"NM\"` in `X_train` with `NaN`. These values likely indicate missing data, so replacing them with `NaN` allows for easier handling of missing values.\n",
    "\n",
    "2. **Identifying Categorical and Numeric Features**  \n",
    "   - The list `categorical_features` is defined with known categorical columns (`Credit_Mix`, `Payment_of_Min_Amount`, `Payment_Behaviour`, `Total_Delayed_Payments`), which are converted to string type to ensure consistent processing as categorical variables.\n",
    "   - `numeric_features` is identified by selecting columns of types `int64` and `float64`, and `categorical_features` is updated by selecting columns of type `object`. This dynamic selection helps to distinguish features that need different preprocessing steps.\n",
    "\n",
    "3. **Handling Missing Values for Numeric Features**  \n",
    "   - `numerical_pipeline` is created to handle missing values and standardize numeric data:\n",
    "     - `SimpleImputer(strategy=\"median\")` fills missing numeric values with the median, ensuring that outliers do not skew the imputed values.\n",
    "     - `StandardScaler()` standardizes numeric values by scaling them to have a mean of 0 and standard deviation of 1, which improves model performance.\n",
    "\n",
    "4. **Handling Missing Values and Encoding for Categorical Features**  \n",
    "   - `categorical_transformer` defines the preprocessing steps for categorical features:\n",
    "     - `SimpleImputer(strategy=\"most_frequent\")` fills missing categorical values with the most common category in each column.\n",
    "     - `OneHotEncoder(handle_unknown=\"ignore\")` converts categorical variables into binary (dummy) variables, creating new columns for each unique category while ignoring unseen categories in new data.\n",
    "\n",
    "5. **Combining Preprocessing Steps**  \n",
    "   - `preprocessor` uses `ColumnTransformer` to apply the defined pipelines:\n",
    "     - The `numerical_pipeline` is applied to columns in `numeric_features`.\n",
    "     - The `categorical_transformer` is applied to columns in `categorical_features`.\n",
    "   - This combined preprocessor ensures that each feature type receives appropriate preprocessing before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify numeric and categorical features\n",
    "\n",
    "\n",
    "X_train.replace(\"-\", np.nan, inplace=True)\n",
    "X_train.replace(\"NM\", np.nan, inplace=True)\n",
    "\n",
    "categorical_features = [\n",
    "    \"Credit_Mix\",\n",
    "    \"Payment_of_Min_Amount\",\n",
    "    \"Payment_Behaviour\",\n",
    "    \"Total_Delayed_Payments\"\n",
    "]\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "# Use SimpleImputer to fill in missing values. Strategies: 'mean' for numerical, 'most_frequent' for categorical.\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "    [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Define the preprocessing steps for numeric features\n",
    "# numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "# Define the preprocessing steps for categorical features\n",
    "# Using OneHotEncoder to convert categorical variables into binary (dummy) variables.\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        ),  #! Can change this step for different encoding methods\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data Columns\n",
    "\n",
    "1. **Printing All Columns**  \n",
    "   - `print(X_train.columns)` prints the names of all columns in the `X_train` DataFrame. This provides an overview of the dataset's structure, allowing you to check which features are available for model training.\n",
    "\n",
    "2. **Printing Numerical Columns**  \n",
    "   - `print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)` prints the names of all columns in `X_train` that have numerical data types (`int64` and `float64`). This helps identify which columns are numeric and should be treated as such during preprocessing.\n",
    "\n",
    "3. **Printing Categorical Columns**  \n",
    "   - `print(X_train.select_dtypes(include=[\"object\"]).columns)` prints the names of all columns in `X_train` with an object data type (typically categorical variables in pandas). This helps identify which columns are categorical and will need encoding during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Income_Annual', 'Base_Salary_PerMonth', 'Total_Bank_Accounts',\n",
      "       'Total_Credit_Cards', 'Rate_Of_Interest', 'Total_Current_Loans',\n",
      "       'Delay_from_due_date', 'Total_Delayed_Payments', 'Credit_Limit',\n",
      "       'Total_Credit_Enquiries', 'Credit_Mix', 'Current_Debt_Outstanding',\n",
      "       'Ratio_Credit_Utilization', 'Credit_History_Age',\n",
      "       'Payment_of_Min_Amount', 'Per_Month_EMI', 'Monthly_Investment',\n",
      "       'Payment_Behaviour', 'Monthly_Balance'],\n",
      "      dtype='object')\n",
      "Index(['Age', 'Income_Annual', 'Base_Salary_PerMonth', 'Total_Bank_Accounts',\n",
      "       'Total_Credit_Cards', 'Rate_Of_Interest', 'Total_Current_Loans',\n",
      "       'Delay_from_due_date', 'Credit_Limit', 'Total_Credit_Enquiries',\n",
      "       'Current_Debt_Outstanding', 'Ratio_Credit_Utilization',\n",
      "       'Credit_History_Age', 'Per_Month_EMI', 'Monthly_Investment',\n",
      "       'Monthly_Balance'],\n",
      "      dtype='object')\n",
      "Index(['Total_Delayed_Payments', 'Credit_Mix', 'Payment_of_Min_Amount',\n",
      "       'Payment_Behaviour'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Print the columns\n",
    "print(X_train.columns)\n",
    "# Print Numerical columns\n",
    "print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
    "# Print Categorical columns\n",
    "print(X_train.select_dtypes(include=[\"object\"]).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering in Test Data\n",
    "\n",
    "1. **Creating Debt-to-Income Ratio**  \n",
    "   - `test_data[\"Debt_Income_Ratio\"]` is created by dividing `Current_Debt_Outstanding` by `Income_Annual`. This new feature provides insight into how much of the individual's annual income is allocated to repaying current debt.\n",
    "\n",
    "2. **Creating Income-to-Credit Limit Ratio**  \n",
    "   - `test_data[\"Income_Credit_Limit_Ratio\"]` is created by dividing `Income_Annual` by `Credit_Limit`. This ratio measures how the individual's income compares to their total credit limit, which can indicate financial leverage.\n",
    "\n",
    "3. **Creating Debt-to-Credit Limit Ratio**  \n",
    "   - `test_data[\"Debt_Credit_Limit_Ratio\"]` is created by dividing `Current_Debt_Outstanding` by `Credit_Limit`. This feature shows how much of the credit limit is being used relative to outstanding debt, which can help evaluate credit utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering in test data\n",
    "test_data[\"Debt_Income_Ratio\"] = (\n",
    "    test_data[\"Current_Debt_Outstanding\"] / test_data[\"Income_Annual\"]\n",
    ")\n",
    "test_data[\"Income_Credit_Limit_Ratio\"] = (\n",
    "    test_data[\"Income_Annual\"] / test_data[\"Credit_Limit\"]\n",
    ")\n",
    "test_data[\"Debt_Credit_Limit_Ratio\"] = (\n",
    "    test_data[\"Current_Debt_Outstanding\"] / test_data[\"Credit_Limit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Type Check and Handling for Mixed Data Types\n",
    "\n",
    "1. **Check for Mixed Data Types in Each Column**  \n",
    "   - The for loop iterates through each column in `X_train` to check for columns with mixed data types. It uses `dropna()` to remove `NaN` values before checking the types of the remaining values.\n",
    "   - `unique_types = set(type(x) for x in X_train[col].dropna())` creates a set of unique types found in each column. If the set has more than one type, it means the column contains mixed data types.\n",
    "   - If mixed types are found, `print(f\"Column '{col}' has mixed types: {unique_types}\")` will output the column name and the different types found in that column.\n",
    "\n",
    "2. **Handling Non-Numeric Values in \"Monthly_Balance\"**  \n",
    "   - The commented-out lines attempt to convert the `Monthly_Balance` column to numeric values using `pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")`. Any non-numeric values will be converted to `NaN` due to the `errors=\"coerce\"` option.\n",
    "   - The following lines are intended to fill any missing values (`NaN`) in the `Monthly_Balance` column with the median value of the column using `X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)`.\n",
    "\n",
    "3. **Uncommented Lines**  \n",
    "   - The code that has been commented out suggests the same approach for filling missing values in both the `X_train` and `test_data` DataFrame's `Monthly_Balance` columns.\n",
    "   - These lines would ensure that any non-numeric values in the `Monthly_Balance` column are converted to `NaN` and then replaced with the median value, improving the consistency and completeness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mixed data types in each column\n",
    "for col in X_train.columns:\n",
    "    unique_types = set(type(x) for x in X_train[col].dropna())\n",
    "    if len(unique_types) > 1:\n",
    "        print(f\"Column '{col}' has mixed types: {unique_types}\")\n",
    "\n",
    "# # Convert all values to NaN whenever str is encountered for Monthly_Balance\n",
    "# X_train[\"Monthly_Balance\"] = pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")\n",
    "\n",
    "# # Fill missing values in 'Monthly_Balance' with the median value\n",
    "# X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)\n",
    "\n",
    "\n",
    "# X_train[\"Monthly_Balance\"] = pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")\n",
    "# # test_data[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "# #     test_data[\"Monthly_Balance\"], errors=\"coerce\"\n",
    "# # )\n",
    "\n",
    "# # Fill missing values in 'Monthly_Balance' with the median value\n",
    "# X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)\n",
    "# # test_data[\"Monthly_Balance\"].fillna(test_data[\"Monthly_Balance\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Numerical Columns in X_train\n",
    "\n",
    "- `print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)` prints the names of all columns in the `X_train` DataFrame that have numerical data types (`int64` and `float64`).\n",
    "- This helps identify which columns contain numeric data, which are typically the features that will be used for model training or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Income_Annual', 'Base_Salary_PerMonth', 'Total_Bank_Accounts',\n",
      "       'Total_Credit_Cards', 'Rate_Of_Interest', 'Total_Current_Loans',\n",
      "       'Delay_from_due_date', 'Credit_Limit', 'Total_Credit_Enquiries',\n",
      "       'Current_Debt_Outstanding', 'Ratio_Credit_Utilization',\n",
      "       'Credit_History_Age', 'Per_Month_EMI', 'Monthly_Investment',\n",
      "       'Monthly_Balance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Apply Preprocessing\n",
    "\n",
    "1. **Apply Transformations to the Training Data**  \n",
    "   - `X_preprocessed = preprocessor.fit_transform(X_train)` applies the preprocessing pipeline (`preprocessor`) to the training data (`X_train`), which includes imputation, scaling, and encoding of features.\n",
    "   - The `fit_transform()` method first fits the preprocessing steps to the data and then transforms it, producing the preprocessed training data in a sparse matrix format.\n",
    "\n",
    "2. **Convert Transformed Data to DataFrame and Assign Column Names**  \n",
    "   - `X_train_df = pd.DataFrame.sparse.from_spmatrix(X_preprocessed)` converts the sparse matrix `X_preprocessed` to a pandas DataFrame (`X_train_df`), which is more suitable for further analysis or model training.\n",
    "   - `X_train_df.columns = preprocessor.get_feature_names_out()` assigns the transformed feature names to the DataFrame columns. `get_feature_names_out()` retrieves the names of all the features after preprocessing (e.g., one-hot encoded columns).\n",
    "\n",
    "3. **Apply Transformations to the Test Data**  \n",
    "   - `test_data_preprocessed = preprocessor.transform(test_data)` applies the same transformations (fit on `X_train`) to the test data (`test_data`). Note that `transform()` is used here, which applies the already fitted transformations without refitting the model.\n",
    "   - `test_data_df = pd.DataFrame.sparse.from_spmatrix(test_data_preprocessed)` converts the transformed test data to a DataFrame format.\n",
    "   - `test_data_df.columns = preprocessor.get_feature_names_out()` assigns the same feature names to the test data columns.\n",
    "\n",
    "# Step 5: Train-Test Split for the Preprocessed Data\n",
    "\n",
    "4. **Train-Test Split**  \n",
    "   - `X_train_1, X_val_1, y_train_1, y_val = train_test_split(X_train_df, y_train, test_size=0.2, random_state=42, stratify=y_train)` splits the preprocessed training data (`X_train_df`) and corresponding labels (`y_train`) into training and validation sets.\n",
    "   - The `test_size=0.2` parameter ensures 20% of the data is used for validation and 80% for training.\n",
    "   - `stratify=y_train` ensures that the class distribution in the target variable (`y_train`) is preserved in both the training and validation sets.\n",
    "\n",
    "5. **Verify the Transformations**  \n",
    "   - `print(\"Transformed Training Data Shape:\", X_train_1.shape)` prints the shape (number of rows and columns) of the transformed training data.\n",
    "   - `print(\"Transformed Validation Data Shape:\", X_val_1.shape)` prints the shape of the validation data.\n",
    "   - `print(\"Transformed Test Data Shape:\", test_data_df.shape)` prints the shape of the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training Data Shape: (64000, 646)\n",
      "Transformed Validation Data Shape: (16000, 646)\n",
      "Transformed Test Data Shape: (20000, 646)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply preprocessing\n",
    "\n",
    "# Apply transformations to the training data\n",
    "X_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Convert to DataFrame and assign column names\n",
    "X_train_df = pd.DataFrame.sparse.from_spmatrix(X_preprocessed)\n",
    "X_train_df.columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Apply transformations to the test data and convert to DataFrame with column names\n",
    "test_data_preprocessed = preprocessor.transform(test_data)\n",
    "test_data_df = pd.DataFrame.sparse.from_spmatrix(test_data_preprocessed)\n",
    "test_data_df.columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 5: Train-Test Split for the preprocessed data\n",
    "X_train_1, X_val_1, y_train_1, y_val = train_test_split(\n",
    "    X_train_df, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Verify the transformations\n",
    "print(\"Transformed Training Data Shape:\", X_train_1.shape)\n",
    "print(\"Transformed Validation Data Shape:\", X_val_1.shape)\n",
    "print(\"Transformed Test Data Shape:\", test_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization and Configuration\n",
    "\n",
    "1. **Create a Dictionary to Store Models**  \n",
    "   - `models = {}` initializes an empty dictionary where different models will be stored by their names as keys.\n",
    "\n",
    "2. **XGBoost Classifier**  \n",
    "   - `model = XGBClassifier(...)` creates an instance of the XGBoost classifier with the following parameters:\n",
    "     - `learning_rate=0.17031088174537234`: Sets the learning rate for the model, controlling the step size at each iteration while moving toward a minimum.\n",
    "     - `max_depth=9`: Defines the maximum depth of a tree in the model.\n",
    "     - `n_estimators=177`: The number of boosting rounds or trees in the model.\n",
    "     - `random_state=42`: Sets a seed for random number generation to ensure reproducibility of the results.\n",
    "     - `eval_metric=\"mlogloss\"`: Specifies the evaluation metric used during training; here it uses the multi-class logarithmic loss.\n",
    "   \n",
    "   - `models[\"XGBoost\"] = model`: Adds the created XGBoost model to the `models` dictionary, using `\"XGBoost\"` as the key.\n",
    "\n",
    "3. **Commented Out Models**  \n",
    "   - Several models are commented out, indicating they were either not used for the current run or are potential alternatives:\n",
    "     - **Random Forest** (`RandomForestClassifier`): A popular ensemble method based on decision trees.\n",
    "     - **K Nearest Neighbors (KNN)** (`KNeighborsClassifier`): A non-parametric classifier based on the closest data points.\n",
    "     - **Logistic Regression** (`LogisticRegression`): A statistical method used for binary classification.\n",
    "     - **Decision Tree Classifier** (`DecisionTreeClassifier`): A tree-based model that splits data into decision nodes.\n",
    "     - **Gaussian Naive Bayes** (`GaussianNB`): A probabilistic classifier based on Bayes' theorem assuming Gaussian distribution for the data.\n",
    "     - **AdaBoost** (`AdaBoostClassifier`): An ensemble learning technique that combines multiple weak classifiers into one strong classifier.\n",
    "     \n",
    "   - Each of these commented-out models is typically added to the `models` dictionary with its respective name as the key.\n",
    "\n",
    "4. **Base Estimator for AdaBoost**  \n",
    "   - `base_estimator = DecisionTreeClassifier(max_depth=6)` sets a base estimator (a decision tree) with a maximum depth of 6, used in the AdaBoost classifier.\n",
    "   - `model = AdaBoostClassifier(...)`: Creates an AdaBoost model using the base estimator, and the model would have been added to the dictionary had it been uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "# model = RandomForestClassifier(n_estimators=300, max_depth=30, random_state=42, min_samples_split =2,\n",
    "# min_samples_leaf=1\n",
    "# )\n",
    "# models[\"Random forest\"] = model\n",
    "model = XGBClassifier(\n",
    "    learning_rate=0.17,\n",
    "    max_depth=9,\n",
    "    n_estimators=177,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\",\n",
    ")\n",
    "# models[\"XGBoost\"] = model\n",
    "# model = CatBoostClassifier(\n",
    "#     iterations=1000,  # Number of boosting iterations\n",
    "#     learning_rate=0.2,  # Learning rate\n",
    "#     depth=10,  # Depth of trees\n",
    "#     random_seed=42,  # Random state for reproducibility\n",
    "#     verbose=100,  # Output training logs every 100 iterations\n",
    "#     l2_leaf_reg=7,  # L2 regularization coefficient\n",
    "# )\n",
    "# models[\"CatBoost\"] = model\n",
    "# model = KNeighborsClassifier(n_neighbors=5)\n",
    "# models[\"K nearest neighbours\"] = model\n",
    "# model = LogisticRegression(random_state=42, max_iter=500)\n",
    "# models[\"Logistic regression\"] = model\n",
    "# model = DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "# models[\"Decision tree classifier\"] = model\n",
    "# model = GaussianNB()\n",
    "# models[\"Gaussian\"] = model\n",
    "# base_estimator = DecisionTreeClassifier(max_depth=6)\n",
    "# model = AdaBoostClassifier(estimator=base_estimator, n_estimators=300, random_state=42)\n",
    "# models[\"Adaboost\"] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Training Labels and Features\n",
    "\n",
    "1. **Display `y_train`**\n",
    "   - `y_train` is the target variable (credit score) for the training data. This variable will be used to train models to predict credit scores.\n",
    "\n",
    "2. **Print the Column Names of `X_train_1`**\n",
    "   - `print(X_train_1.columns)` prints the column names of the preprocessed training features (`X_train_1`) to confirm that all expected columns are present after preprocessing.\n",
    "\n",
    "3. **Print `y_train`**\n",
    "   - `print(y_train)` outputs the `y_train` series to verify that it contains the expected labels (credit scores).\n",
    "\n",
    "4. **Check Shape of `y_train`**\n",
    "   - The comment `# y_train should have 1 column of credit score` is a reminder that `y_train` should contain only a single column representing credit scores, as it is a classification target for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['num__Age', 'num__Income_Annual', 'num__Base_Salary_PerMonth',\n",
      "       'num__Total_Bank_Accounts', 'num__Total_Credit_Cards',\n",
      "       'num__Rate_Of_Interest', 'num__Total_Current_Loans',\n",
      "       'num__Delay_from_due_date', 'num__Credit_Limit',\n",
      "       'num__Total_Credit_Enquiries',\n",
      "       ...\n",
      "       'cat__Payment_of_Min_Amount_No', 'cat__Payment_of_Min_Amount_Yes',\n",
      "       'cat__Payment_of_Min_Amount_nan', 'cat__Payment_Behaviour_!@9#%8',\n",
      "       'cat__Payment_Behaviour_High_spent_Large_value_payments',\n",
      "       'cat__Payment_Behaviour_High_spent_Medium_value_payments',\n",
      "       'cat__Payment_Behaviour_High_spent_Small_value_payments',\n",
      "       'cat__Payment_Behaviour_Low_spent_Large_value_payments',\n",
      "       'cat__Payment_Behaviour_Low_spent_Medium_value_payments',\n",
      "       'cat__Payment_Behaviour_Low_spent_Small_value_payments'],\n",
      "      dtype='object', length=646)\n",
      "0        2\n",
      "1        2\n",
      "2        2\n",
      "3        2\n",
      "4        1\n",
      "        ..\n",
      "79995    1\n",
      "79996    1\n",
      "79997    1\n",
      "79998    2\n",
      "79999    2\n",
      "Name: Credit_Score, Length: 80000, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "y_train\n",
    "print(X_train_1.columns)\n",
    "print(y_train)\n",
    "# y_train should have 1 column of credit score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for XGBoost Classifier Using Optuna\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "   - `optuna`: For conducting hyperparameter optimization.\n",
    "   - `XGBClassifier` from `xgboost`: The machine learning model used for classification.\n",
    "   - `cross_val_score` from `sklearn.model_selection`: Used for performing cross-validation to evaluate model performance.\n",
    "\n",
    "2. **Define Objective Function for Optuna**\n",
    "   - The `objective` function defines the process Optuna will use to test and evaluate different parameter combinations.\n",
    "   - `params` dictionary specifies the search space for hyperparameters:\n",
    "     - `learning_rate`: A float between 0.01 and 0.2.\n",
    "     - `max_depth`: An integer between 3 and 10, controlling the depth of each tree.\n",
    "     - `n_estimators`: An integer between 100 and 500, determining the number of trees.\n",
    "     - `subsample`: A float between 0.5 and 1.0, which defines the fraction of samples used for training each tree.\n",
    "     - `colsample_bytree`: A float between 0.5 and 1.0, which specifies the fraction of features used by each tree.\n",
    "\n",
    "3. **Initialize the XGBoost Classifier with Trial Parameters**\n",
    "   - `model = XGBClassifier(**params, random_state=42, eval_metric=\"mlogloss\")`: Creates an instance of the XGBoost model with parameters generated by the current Optuna trial.\n",
    "\n",
    "4. **Cross-Validation to Evaluate Model**\n",
    "   - `accuracy = cross_val_score(model, X_train_1, y_train_1, cv=5, scoring=\"accuracy\").mean()` performs 5-fold cross-validation on the model, returning the mean accuracy score.\n",
    "\n",
    "5. **Run Bayesian Optimization with Optuna**\n",
    "   - `study = optuna.create_study(direction=\"maximize\")`: Initializes the optimization process with the goal of maximizing accuracy.\n",
    "   - `study.optimize(objective, n_trials=50)`: Runs the optimization for 50 trials.\n",
    "\n",
    "6. **Display Results**\n",
    "   - After optimization, print the best parameters and best accuracy:\n",
    "     - `print(\"Best parameters found: \", study.best_params)`\n",
    "     - `print(\"Best accuracy: \", study.best_value)`\n",
    "\n",
    "   The commented lines provide logs from sample trials showing trial results and the parameters that yielded the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 31\u001b[0m\n\u001b[0;32m     19\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     20\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb,\n\u001b[0;32m     21\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Use all available cores\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Fit the random search to the data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Print the best parameters and the best score\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects5thSemester\\Machine Learning(Course)\\ML-course-project-credit-rank\\mlenv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid for RandomizedSearch\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\": [3, 5, 10, 15, 20],\n",
    "    \"n_estimators\": [300, 400, 500, 600, 700],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb = XGBClassifier(random_state=42, eval_metric=\"mlogloss\")\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=2,  # Number of parameter combinations to try\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_val_1)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    # Initialize the classifier with trial parameters\n",
    "    model = XGBClassifier(**params, random_state=42, eval_metric=\"mlogloss\")\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    accuracy = cross_val_score(model, X_train_1, y_train_1, cv=5, scoring=\"accuracy\").mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization with Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# [I 2024-11-03 16:53:08,619] Trial 0 finished with value: 0.7062031249999999 and parameters: {'learning_rate': 0.050893568127103876, 'max_depth': 4, 'n_estimators': 268, 'subsample': 0.6738283278380766, 'colsample_bytree': 0.6915738416826347}. Best is trial 0 with value: 0.7062031249999999.\n",
    "\n",
    "# [I 2024-11-03 16:58:00,159] Trial 2 finished with value: 0.7632656250000001 and parameters: {'learning_rate': 0.17031088174537234, 'max_depth': 9, 'n_estimators': 177, 'subsample': 0.8174475788946926, 'colsample_bytree': 0.5291010066051618}. Best is trial 2 with value: 0.7632656250000001.\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters found: \", study.best_params)\n",
    "print(\"Best accuracy: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training, Prediction, and Submission File Creation for Each Model in `models` Dictionary\n",
    "\n",
    "1. **Loop Through Models**\n",
    "   - `for key, value in models.items()`: Iterates through each model in the `models` dictionary, where `key` represents the model's name and `value` is the model instance.\n",
    "\n",
    "2. **Try-Except Block for Error Handling**\n",
    "   - Wraps the pipeline setup and predictions within a try-except block to catch and report any potential errors during the process.\n",
    "\n",
    "3. **Define Model Pipeline**\n",
    "   - `pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", value)])`: Creates a pipeline that combines data preprocessing (`preprocessor`) and the model (`classifier`), which allows both steps to be executed sequentially.\n",
    "\n",
    "4. **Fit the Pipeline on Training Data**\n",
    "   - `pipeline.fit(X_train, y_train)`: Trains the pipeline on the training data, applying the preprocessing steps followed by fitting the model.\n",
    "\n",
    "5. **Make Predictions on Test Data**\n",
    "   - `test_predictions = pipeline.predict(test_data)`: Generates predictions for the test dataset using the fitted pipeline.\n",
    "\n",
    "6. **Convert Predictions Back to Original Labels**\n",
    "   - `test_predictions_labels = label_encoder.inverse_transform(test_predictions)`: Converts encoded predictions back to the original label format using a `label_encoder`.\n",
    "\n",
    "7. **Prepare the Submission File**\n",
    "   - `submission = pd.DataFrame({\"ID\": test_ids, \"Credit_Score\": test_predictions_labels})`: Creates a DataFrame for submission with test IDs and predicted credit scores.\n",
    "   - `submission.to_csv(f\"submission_{key}.csv\", index=False)`: Saves the DataFrame as a CSV file with the model's name included in the filename.\n",
    "\n",
    "8. **Print Success Message**\n",
    "   - `print(f\"Submission file 'submission_{key}.csv' created successfully!\")`: Confirms successful file creation.\n",
    "\n",
    "9. **Handle Exceptions**\n",
    "   - In case of an error, prints an error message with details of the exception: `print(f\"Error : {e}\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in models.items():\n",
    "    try:  \n",
    "        # Define model pipeline\n",
    "        pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", value)])\n",
    "\n",
    "        # Fit the pipeline on training data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        test_predictions = pipeline.predict(test_data)\n",
    "\n",
    "        # Convert predictions back to original labels\n",
    "        test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "        # test_predictions_encoded = label_encoder.transform(test_predictions)\n",
    "        # Prepare the submission file\n",
    "        submission = pd.DataFrame(\n",
    "            {\"ID\": test_ids, \"Credit_Score\": test_predictions_labels}\n",
    "        )\n",
    "        submission.to_csv(f\"submission_{key}.csv\", index=False)\n",
    "\n",
    "        print(f\"Submission file 'submission_{key}.csv' created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
