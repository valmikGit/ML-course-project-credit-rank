{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "1. **Load the Dataset**  \n",
    "   - The training dataset is loaded from `train.csv` and stored in `train_data`.\n",
    "   - The test dataset is also loaded from `test.csv` and stored in `test_data`.\n",
    "   - The `ID` column is separately loaded from the test dataset to create `test_ids`, which will likely be used for referencing test predictions later.\n",
    "\n",
    "2. **Analyze the Dataset**  \n",
    "   - Several columns that are not essential for modeling, including `ID`, `Customer_ID`, `Month`, `Name`, `Profession`, `Number`, and `Loan_Type`, are removed from the `train_data` DataFrame. This simplifies the dataset by retaining only the most relevant features for the predictive model.\n",
    "   - The `train_data.head()` function displays the first few rows of the modified training dataset, giving a quick preview of the remaining columns.\n",
    "\n",
    "3. **Categorical Columns**  \n",
    "   - Certain columns, such as `Loan_Type`, `Credit_Mix`, `Payment_of_Min_Amount`, `Payment_Behaviour`, and `Credit_Score`, contain categorical data.\n",
    "   - The `Credit_Score` column, which indicates the creditworthiness of customers, is identified as the target variable for the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_ids = pd.read_csv(\"test.csv\")[\"ID\"]\n",
    "# Analyze your dataset\n",
    "# ID, Customer_ID, Month, Name, Age, Profession, Number are columns to be dropped\n",
    "train_data = train_data.drop(columns=[\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"Profession\", \"Number\", \"Loan_Type\"])\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# Loan_Type, Credit_Mix, Payment_of_Min_Amount, Payment_Behaviour , and Credit_Score are categorical columns\n",
    "# Credit_Score is the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "1. **Fill Missing Values with Median**  \n",
    "   - The `fillna()` function is used to replace any missing values in the `train_data` DataFrame with the median of each column.\n",
    "   - The `train_data.median(numeric_only=True)` calculation returns the median values of the numerical columns in `train_data`. This approach minimizes the impact of outliers, making it a robust choice for imputation.\n",
    "   - `inplace=True` modifies `train_data` directly, updating it without needing to reassign it to a new variable.\n",
    "\n",
    "2. **Preview Data After Imputation**  \n",
    "   - The `print(train_data.head())` statement displays the first few rows of `train_data` to verify that missing values have been replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with median in train data\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Type Conversion\n",
    "\n",
    "1. **Removing Underscores and Converting Columns to Numeric**  \n",
    "   - A `for` loop iterates over each column in the specified list of columns that are expected to contain numeric data but may have underscore characters (e.g., `\"_\"`) in the values, which can interfere with numeric conversion.\n",
    "   - For each column:\n",
    "     - In `train_data`, the column values are first converted to strings and any underscores are removed using `str.replace(\"_\", \"\", regex=False)`.\n",
    "     - After removing underscores, `pd.to_numeric()` converts the cleaned string values to numeric values.\n",
    "     - The `errors=\"coerce\"` parameter ensures that any non-numeric values that remain after removing underscores are replaced with `NaN`, allowing for consistent data handling.\n",
    "   - The same cleaning and conversion process is applied to each corresponding column in `test_data`.\n",
    "\n",
    "2. **Preview of Additional Columns to Consider**  \n",
    "   - The comment block below the code lists additional columns that may also require cleaning or conversion, such as `Income_Annual`, `Base_Salary_PerMonth`, and `Current_Debt_Outstanding`. These columns may need further processing to ensure they are in the correct format for analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric after removing any underscores.This was done as I saw a value in Total_Current_Loans with an underscore.\n",
    "for col in [\"Total_Current_Loans\", \"Current_Debt_Outstanding\", \"Income_Annual\", \"Credit_Limit\", \"Age\", \"Total_Credit_Cards\", \"Total_Bank_Accounts\", \"Delay_from_due_date\"]:\n",
    "    train_data[col] = pd.to_numeric(\n",
    "        train_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "    test_data[col] = pd.to_numeric(\n",
    "        test_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    "    )\n",
    "train_data\n",
    "# Income_Annual, Base_Salary_PerMonth,Current_Debt_Outstanding,Ratio_Credit_Utilization, Per_Month_EMI, Monthly_Investment\n",
    "\n",
    "# 'Total_Delayed_Payments', 'Credit_Mix',\n",
    "#        'Credit_History_Age', 'Payment_of_Min_Amount', 'Monthly_Investment',\n",
    "#        'Payment_Behaviour', 'Monthly_Balance'],\n",
    "#       dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting `Credit_History_Age` to Numeric Format\n",
    "\n",
    "1. **Extracting Numerical Part from `Credit_History_Age`**  \n",
    "   - The `Credit_History_Age` column contains values in a text format such as `\"1 Years and 2 Months\"`. To use this data numerically, we extract the numeric portion representing the years.\n",
    "   - `str.extract(\"(\\d+)\")` uses a regular expression to capture only the numeric part (the number of years) from each entry, ignoring the rest of the text. `(\\d+)` matches one or more digits.\n",
    "\n",
    "2. **Converting Extracted Data to Float**  \n",
    "   - After extraction, `.astype(float)` converts the resulting values to a float data type, ensuring the column is in numeric format and can be used in calculations or models.\n",
    "   - This process is applied to both `train_data` and `test_data`, ensuring consistency across both datasets.\n",
    "\n",
    "3. **Previewing the Converted Column**  \n",
    "   - The final line displays the transformed `Credit_History_Age` column from `train_data`, allowing verification of the conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert credit_history_age to numeric\n",
    "# Format : 1 Years and 2 Months\n",
    "train_data[\"Credit_History_Age\"] = train_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "test_data[\"Credit_History_Age\"] = test_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "train_data[\"Credit_History_Age\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Infinity Values and Encoding the Target Variable\n",
    "\n",
    "1. **Replacing Infinity Values**  \n",
    "   - `train_data.replace([np.inf, -np.inf], np.nan, inplace=True)` searches for any infinity (`np.inf`) or negative infinity (`-np.inf`) values in `train_data` and replaces them with `NaN`. This ensures that infinity values, which may disrupt model training, are handled appropriately.\n",
    "\n",
    "2. **Filling Missing Values with Median**  \n",
    "   - After replacing infinity values with `NaN`, `train_data.fillna(train_data.median(numeric_only=True), inplace=True)` fills any remaining `NaN` values (including those from the previous step) with the median of each numeric column. This step ensures that the dataset remains complete and without missing values.\n",
    "\n",
    "3. **Label Encoding the Target Variable**  \n",
    "   - `LabelEncoder()` is used to convert the target column `Credit_Score` into numerical format. This is especially useful if `Credit_Score` is categorical (e.g., \"Good\", \"Average\", \"Bad\").\n",
    "   - `label_encoder.fit_transform(train_data[\"Credit_Score\"])` transforms the categories in `Credit_Score` to numerical labels, which is necessary for most machine learning models.\n",
    "   - The modified `Credit_Score` column in `train_data` now contains integer values representing each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity values in train data\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Label encode the target variable in train data\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"Credit_Score\"] = label_encoder.fit_transform(train_data[\"Credit_Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training Features and Labels\n",
    "\n",
    "1. **Separating Features (X_train)**  \n",
    "   - `X_train` is created by dropping the target column, `Credit_Score`, from `train_data`. This DataFrame now contains only the feature columns that will be used as inputs for the model.\n",
    "\n",
    "2. **Separating Labels (y_train)**  \n",
    "   - `y_train` is created by selecting only the `Credit_Score` column from `train_data`. This Series represents the target variable, containing the labels that the model will learn to predict.\n",
    "\n",
    "Together, `X_train` and `y_train` form the feature set and label set for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training features and labels\n",
    "X_train = train_data.drop(columns=\"Credit_Score\")\n",
    "y_train = train_data[\"Credit_Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering in `train_data`\n",
    "\n",
    "1. **Creating `Debt_Income_Ratio`**  \n",
    "   - This new feature calculates the ratio of `Current_Debt_Outstanding` to `Income_Annual`. It provides insight into the debt burden relative to income, which can be an important indicator of credit risk.\n",
    "\n",
    "2. **Creating `Income_Credit_Limit_Ratio`**  \n",
    "   - This feature is the ratio of `Income_Annual` to `Credit_Limit`. It helps assess the income capacity against the available credit limit, which can indicate how likely a customer might be to max out their credit.\n",
    "\n",
    "3. **Creating `Debt_Credit_Limit_Ratio`**  \n",
    "   - This feature is calculated as the ratio of `Current_Debt_Outstanding` to `Credit_Limit`. It measures the proportion of debt relative to the available credit limit, which can indicate the extent to which a customer relies on credit.\n",
    "\n",
    "These new features enhance the dataset by providing additional, potentially predictive variables that may improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering in train data\n",
    "train_data[\"Debt_Income_Ratio\"] = (\n",
    "    train_data[\"Current_Debt_Outstanding\"] / train_data[\"Income_Annual\"]\n",
    ")\n",
    "train_data[\"Income_Credit_Limit_Ratio\"] = (\n",
    "    train_data[\"Income_Annual\"] / train_data[\"Credit_Limit\"]\n",
    ")\n",
    "train_data[\"Debt_Credit_Limit_Ratio\"] = (\n",
    "    train_data[\"Current_Debt_Outstanding\"] / train_data[\"Credit_Limit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Converting Monthly Balance and Monthly Investment Columns\n",
    "\n",
    "1. **Inspecting Data Columns**  \n",
    "   - The code includes commented-out lines to print the `Monthly_Balance` and `Monthly_Investment` columns and identify object-type columns in `X_train`. These checks can help understand the initial data format and identify any issues before conversion.\n",
    "\n",
    "2. **Converting `Monthly_Balance` to Numeric**  \n",
    "   - `X_train[\"Monthly_Balance\"]` is converted to numeric by first converting it to a string, removing any underscores (`_`) that may be present, and then using `pd.to_numeric` for conversion.\n",
    "   - `errors=\"coerce\"` replaces any non-numeric values with `NaN`, ensuring the column is consistently numeric.\n",
    "\n",
    "3. **Converting `Monthly_Investment` to Numeric**  \n",
    "   - Similarly, `X_train[\"Monthly_Investment\"]` is converted by removing underscores and coercing any invalid values to `NaN`.\n",
    "\n",
    "4. **Filling Missing Values with Median**  \n",
    "   - Any `NaN` values in `X_train` resulting from the conversion process are filled with the median values of the numeric columns to maintain data completeness.\n",
    "\n",
    "5. **Applying the Same Cleaning Steps to `test_data`**  \n",
    "   - The `test_data` DataFrame undergoes the same cleaning and conversion process for `Monthly_Balance` and `Monthly_Investment`, ensuring both datasets are treated consistently.\n",
    "\n",
    "6. **Printing the Converted Columns**  \n",
    "   - Finally, the code prints `Monthly_Investment` and `Monthly_Balance` from `X_train` to verify the transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Monthly_Balance column\n",
    "# print(X_train[\"Monthly_Investment\"])\n",
    "# print(X_train.select_dtypes(include=[\"object\"]).columns)\n",
    "# Replace all str values in Monthly_Balance, Monthly_Investment\n",
    "\n",
    "X_train[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "    X_train[\"Monthly_Balance\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "X_train[\"Monthly_Investment\"] = pd.to_numeric(\n",
    "    X_train[\"Monthly_Investment\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Fill missing values with median for Monthly_Balance and Monthly_Investment\n",
    "X_train.fillna(X_train.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Same for test data\n",
    "test_data[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "    test_data[\"Monthly_Balance\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "test_data[\"Monthly_Investment\"] = pd.to_numeric(\n",
    "    test_data[\"Monthly_Investment\"].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "test_data.fillna(test_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# print Monthly_Balance column\n",
    "print(X_train[\"Monthly_Investment\"], X_train[\"Monthly_Balance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Training\n",
    "\n",
    "1. **Replacing Placeholder Values with `NaN`**  \n",
    "   - `X_train.replace(\"-\", np.nan, inplace=True)` and `X_train.replace(\"NM\", np.nan, inplace=True)` replace any instances of `\"-\"` and `\"NM\"` in `X_train` with `NaN`. These values likely indicate missing data, so replacing them with `NaN` allows for easier handling of missing values.\n",
    "\n",
    "2. **Identifying Categorical and Numeric Features**  \n",
    "   - The list `categorical_features` is defined with known categorical columns (`Credit_Mix`, `Payment_of_Min_Amount`, `Payment_Behaviour`, `Total_Delayed_Payments`), which are converted to string type to ensure consistent processing as categorical variables.\n",
    "   - `numeric_features` is identified by selecting columns of types `int64` and `float64`, and `categorical_features` is updated by selecting columns of type `object`. This dynamic selection helps to distinguish features that need different preprocessing steps.\n",
    "\n",
    "3. **Handling Missing Values for Numeric Features**  \n",
    "   - `numerical_pipeline` is created to handle missing values and standardize numeric data:\n",
    "     - `SimpleImputer(strategy=\"median\")` fills missing numeric values with the median, ensuring that outliers do not skew the imputed values.\n",
    "     - `StandardScaler()` standardizes numeric values by scaling them to have a mean of 0 and standard deviation of 1, which improves model performance.\n",
    "\n",
    "4. **Handling Missing Values and Encoding for Categorical Features**  \n",
    "   - `categorical_transformer` defines the preprocessing steps for categorical features:\n",
    "     - `SimpleImputer(strategy=\"most_frequent\")` fills missing categorical values with the most common category in each column.\n",
    "     - `OneHotEncoder(handle_unknown=\"ignore\")` converts categorical variables into binary (dummy) variables, creating new columns for each unique category while ignoring unseen categories in new data.\n",
    "\n",
    "5. **Combining Preprocessing Steps**  \n",
    "   - `preprocessor` uses `ColumnTransformer` to apply the defined pipelines:\n",
    "     - The `numerical_pipeline` is applied to columns in `numeric_features`.\n",
    "     - The `categorical_transformer` is applied to columns in `categorical_features`.\n",
    "   - This combined preprocessor ensures that each feature type receives appropriate preprocessing before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify numeric and categorical features\n",
    "\n",
    "\n",
    "X_train.replace(\"-\", np.nan, inplace=True)\n",
    "X_train.replace(\"NM\", np.nan, inplace=True)\n",
    "\n",
    "categorical_features = [\n",
    "    \"Credit_Mix\",\n",
    "    \"Payment_of_Min_Amount\",\n",
    "    \"Payment_Behaviour\",\n",
    "    \"Total_Delayed_Payments\"\n",
    "]\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "# Use SimpleImputer to fill in missing values. Strategies: 'mean' for numerical, 'most_frequent' for categorical.\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "    [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Define the preprocessing steps for numeric features\n",
    "# numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "# Define the preprocessing steps for categorical features\n",
    "# Using OneHotEncoder to convert categorical variables into binary (dummy) variables.\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        ),  #! Can change this step for different encoding methods\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data Columns\n",
    "\n",
    "1. **Printing All Columns**  \n",
    "   - `print(X_train.columns)` prints the names of all columns in the `X_train` DataFrame. This provides an overview of the dataset's structure, allowing you to check which features are available for model training.\n",
    "\n",
    "2. **Printing Numerical Columns**  \n",
    "   - `print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)` prints the names of all columns in `X_train` that have numerical data types (`int64` and `float64`). This helps identify which columns are numeric and should be treated as such during preprocessing.\n",
    "\n",
    "3. **Printing Categorical Columns**  \n",
    "   - `print(X_train.select_dtypes(include=[\"object\"]).columns)` prints the names of all columns in `X_train` with an object data type (typically categorical variables in pandas). This helps identify which columns are categorical and will need encoding during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the columns\n",
    "print(X_train.columns)\n",
    "# Print Numerical columns\n",
    "print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
    "# Print Categorical columns\n",
    "print(X_train.select_dtypes(include=[\"object\"]).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering in Test Data\n",
    "\n",
    "1. **Creating Debt-to-Income Ratio**  \n",
    "   - `test_data[\"Debt_Income_Ratio\"]` is created by dividing `Current_Debt_Outstanding` by `Income_Annual`. This new feature provides insight into how much of the individual's annual income is allocated to repaying current debt.\n",
    "\n",
    "2. **Creating Income-to-Credit Limit Ratio**  \n",
    "   - `test_data[\"Income_Credit_Limit_Ratio\"]` is created by dividing `Income_Annual` by `Credit_Limit`. This ratio measures how the individual's income compares to their total credit limit, which can indicate financial leverage.\n",
    "\n",
    "3. **Creating Debt-to-Credit Limit Ratio**  \n",
    "   - `test_data[\"Debt_Credit_Limit_Ratio\"]` is created by dividing `Current_Debt_Outstanding` by `Credit_Limit`. This feature shows how much of the credit limit is being used relative to outstanding debt, which can help evaluate credit utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering in test data\n",
    "test_data[\"Debt_Income_Ratio\"] = (\n",
    "    test_data[\"Current_Debt_Outstanding\"] / test_data[\"Income_Annual\"]\n",
    ")\n",
    "test_data[\"Income_Credit_Limit_Ratio\"] = (\n",
    "    test_data[\"Income_Annual\"] / test_data[\"Credit_Limit\"]\n",
    ")\n",
    "test_data[\"Debt_Credit_Limit_Ratio\"] = (\n",
    "    test_data[\"Current_Debt_Outstanding\"] / test_data[\"Credit_Limit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Type Check and Handling for Mixed Data Types\n",
    "\n",
    "1. **Check for Mixed Data Types in Each Column**  \n",
    "   - The for loop iterates through each column in `X_train` to check for columns with mixed data types. It uses `dropna()` to remove `NaN` values before checking the types of the remaining values.\n",
    "   - `unique_types = set(type(x) for x in X_train[col].dropna())` creates a set of unique types found in each column. If the set has more than one type, it means the column contains mixed data types.\n",
    "   - If mixed types are found, `print(f\"Column '{col}' has mixed types: {unique_types}\")` will output the column name and the different types found in that column.\n",
    "\n",
    "2. **Handling Non-Numeric Values in \"Monthly_Balance\"**  \n",
    "   - The commented-out lines attempt to convert the `Monthly_Balance` column to numeric values using `pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")`. Any non-numeric values will be converted to `NaN` due to the `errors=\"coerce\"` option.\n",
    "   - The following lines are intended to fill any missing values (`NaN`) in the `Monthly_Balance` column with the median value of the column using `X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)`.\n",
    "\n",
    "3. **Uncommented Lines**  \n",
    "   - The code that has been commented out suggests the same approach for filling missing values in both the `X_train` and `test_data` DataFrame's `Monthly_Balance` columns.\n",
    "   - These lines would ensure that any non-numeric values in the `Monthly_Balance` column are converted to `NaN` and then replaced with the median value, improving the consistency and completeness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mixed data types in each column\n",
    "for col in X_train.columns:\n",
    "    unique_types = set(type(x) for x in X_train[col].dropna())\n",
    "    if len(unique_types) > 1:\n",
    "        print(f\"Column '{col}' has mixed types: {unique_types}\")\n",
    "\n",
    "# # Convert all values to NaN whenever str is encountered for Monthly_Balance\n",
    "# X_train[\"Monthly_Balance\"] = pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")\n",
    "\n",
    "# # Fill missing values in 'Monthly_Balance' with the median value\n",
    "# X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)\n",
    "\n",
    "\n",
    "# X_train[\"Monthly_Balance\"] = pd.to_numeric(X_train[\"Monthly_Balance\"], errors=\"coerce\")\n",
    "# # test_data[\"Monthly_Balance\"] = pd.to_numeric(\n",
    "# #     test_data[\"Monthly_Balance\"], errors=\"coerce\"\n",
    "# # )\n",
    "\n",
    "# # Fill missing values in 'Monthly_Balance' with the median value\n",
    "# X_train[\"Monthly_Balance\"].fillna(X_train[\"Monthly_Balance\"].median(), inplace=True)\n",
    "# # test_data[\"Monthly_Balance\"].fillna(test_data[\"Monthly_Balance\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Numerical Columns in X_train\n",
    "\n",
    "- `print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)` prints the names of all columns in the `X_train` DataFrame that have numerical data types (`int64` and `float64`).\n",
    "- This helps identify which columns contain numeric data, which are typically the features that will be used for model training or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Apply Preprocessing\n",
    "\n",
    "1. **Apply Transformations to the Training Data**  \n",
    "   - `X_preprocessed = preprocessor.fit_transform(X_train)` applies the preprocessing pipeline (`preprocessor`) to the training data (`X_train`), which includes imputation, scaling, and encoding of features.\n",
    "   - The `fit_transform()` method first fits the preprocessing steps to the data and then transforms it, producing the preprocessed training data in a sparse matrix format.\n",
    "\n",
    "2. **Convert Transformed Data to DataFrame and Assign Column Names**  \n",
    "   - `X_train_df = pd.DataFrame.sparse.from_spmatrix(X_preprocessed)` converts the sparse matrix `X_preprocessed` to a pandas DataFrame (`X_train_df`), which is more suitable for further analysis or model training.\n",
    "   - `X_train_df.columns = preprocessor.get_feature_names_out()` assigns the transformed feature names to the DataFrame columns. `get_feature_names_out()` retrieves the names of all the features after preprocessing (e.g., one-hot encoded columns).\n",
    "\n",
    "3. **Apply Transformations to the Test Data**  \n",
    "   - `test_data_preprocessed = preprocessor.transform(test_data)` applies the same transformations (fit on `X_train`) to the test data (`test_data`). Note that `transform()` is used here, which applies the already fitted transformations without refitting the model.\n",
    "   - `test_data_df = pd.DataFrame.sparse.from_spmatrix(test_data_preprocessed)` converts the transformed test data to a DataFrame format.\n",
    "   - `test_data_df.columns = preprocessor.get_feature_names_out()` assigns the same feature names to the test data columns.\n",
    "\n",
    "# Step 5: Train-Test Split for the Preprocessed Data\n",
    "\n",
    "4. **Train-Test Split**  \n",
    "   - `X_train_1, X_val_1, y_train_1, y_val = train_test_split(X_train_df, y_train, test_size=0.2, random_state=42, stratify=y_train)` splits the preprocessed training data (`X_train_df`) and corresponding labels (`y_train`) into training and validation sets.\n",
    "   - The `test_size=0.2` parameter ensures 20% of the data is used for validation and 80% for training.\n",
    "   - `stratify=y_train` ensures that the class distribution in the target variable (`y_train`) is preserved in both the training and validation sets.\n",
    "\n",
    "5. **Verify the Transformations**  \n",
    "   - `print(\"Transformed Training Data Shape:\", X_train_1.shape)` prints the shape (number of rows and columns) of the transformed training data.\n",
    "   - `print(\"Transformed Validation Data Shape:\", X_val_1.shape)` prints the shape of the validation data.\n",
    "   - `print(\"Transformed Test Data Shape:\", test_data_df.shape)` prints the shape of the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply preprocessing\n",
    "\n",
    "# Apply transformations to the training data\n",
    "X_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Convert to DataFrame and assign column names\n",
    "X_train_df = pd.DataFrame.sparse.from_spmatrix(X_preprocessed)\n",
    "X_train_df.columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Apply transformations to the test data and convert to DataFrame with column names\n",
    "test_data_preprocessed = preprocessor.transform(test_data)\n",
    "test_data_df = pd.DataFrame.sparse.from_spmatrix(test_data_preprocessed)\n",
    "test_data_df.columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 5: Train-Test Split for the preprocessed data\n",
    "X_train_1, X_val_1, y_train_1, y_val = train_test_split(\n",
    "    X_train_df, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Verify the transformations\n",
    "print(\"Transformed Training Data Shape:\", X_train_1.shape)\n",
    "print(\"Transformed Validation Data Shape:\", X_val_1.shape)\n",
    "print(\"Transformed Test Data Shape:\", test_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization and Configuration\n",
    "\n",
    "1. **Create a Dictionary to Store Models**  \n",
    "   - `models = {}` initializes an empty dictionary where different models will be stored by their names as keys.\n",
    "\n",
    "2. **XGBoost Classifier**  \n",
    "   - `model = XGBClassifier(...)` creates an instance of the XGBoost classifier with the following parameters:\n",
    "     - `learning_rate=0.17031088174537234`: Sets the learning rate for the model, controlling the step size at each iteration while moving toward a minimum.\n",
    "     - `max_depth=9`: Defines the maximum depth of a tree in the model.\n",
    "     - `n_estimators=177`: The number of boosting rounds or trees in the model.\n",
    "     - `random_state=42`: Sets a seed for random number generation to ensure reproducibility of the results.\n",
    "     - `eval_metric=\"mlogloss\"`: Specifies the evaluation metric used during training; here it uses the multi-class logarithmic loss.\n",
    "   \n",
    "   - `models[\"XGBoost\"] = model`: Adds the created XGBoost model to the `models` dictionary, using `\"XGBoost\"` as the key.\n",
    "\n",
    "3. **Commented Out Models**  \n",
    "   - Several models are commented out, indicating they were either not used for the current run or are potential alternatives:\n",
    "     - **Random Forest** (`RandomForestClassifier`): A popular ensemble method based on decision trees.\n",
    "     - **K Nearest Neighbors (KNN)** (`KNeighborsClassifier`): A non-parametric classifier based on the closest data points.\n",
    "     - **Logistic Regression** (`LogisticRegression`): A statistical method used for binary classification.\n",
    "     - **Decision Tree Classifier** (`DecisionTreeClassifier`): A tree-based model that splits data into decision nodes.\n",
    "     - **Gaussian Naive Bayes** (`GaussianNB`): A probabilistic classifier based on Bayes' theorem assuming Gaussian distribution for the data.\n",
    "     - **AdaBoost** (`AdaBoostClassifier`): An ensemble learning technique that combines multiple weak classifiers into one strong classifier.\n",
    "     \n",
    "   - Each of these commented-out models is typically added to the `models` dictionary with its respective name as the key.\n",
    "\n",
    "4. **Base Estimator for AdaBoost**  \n",
    "   - `base_estimator = DecisionTreeClassifier(max_depth=6)` sets a base estimator (a decision tree) with a maximum depth of 6, used in the AdaBoost classifier.\n",
    "   - `model = AdaBoostClassifier(...)`: Creates an AdaBoost model using the base estimator, and the model would have been added to the dictionary had it been uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "# model = RandomForestClassifier(n_estimators=300, max_depth=6, random_state=42)\n",
    "# models[\"Random forest\"] = model\n",
    "# model = XGBClassifier(\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=6,\n",
    "#     n_estimators=300,\n",
    "#     random_state=42,\n",
    "#     eval_metric=\"mlogloss\",\n",
    "# )\n",
    "model = XGBClassifier(\n",
    "    learning_rate=0.17031088174537234,\n",
    "    max_depth=9,\n",
    "    n_estimators=177,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\",\n",
    ")\n",
    "models[\"XGBoost\"] = model\n",
    "# model = KNeighborsClassifier(n_neighbors=5)\n",
    "# models[\"K nearest neighbours\"] = model\n",
    "# model = LogisticRegression(random_state=42, max_iter=500)\n",
    "# models[\"Logistic regression\"] = model\n",
    "# model = DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "# models[\"Decision tree classifier\"] = model\n",
    "# model = GaussianNB()\n",
    "# models[\"Gaussian\"] = model\n",
    "# base_estimator = DecisionTreeClassifier(max_depth=6)\n",
    "# model = AdaBoostClassifier(estimator=base_estimator, n_estimators=300, random_state=42)\n",
    "# models[\"Adaboost\"] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Training Labels and Features\n",
    "\n",
    "1. **Display `y_train`**\n",
    "   - `y_train` is the target variable (credit score) for the training data. This variable will be used to train models to predict credit scores.\n",
    "\n",
    "2. **Print the Column Names of `X_train_1`**\n",
    "   - `print(X_train_1.columns)` prints the column names of the preprocessed training features (`X_train_1`) to confirm that all expected columns are present after preprocessing.\n",
    "\n",
    "3. **Print `y_train`**\n",
    "   - `print(y_train)` outputs the `y_train` series to verify that it contains the expected labels (credit scores).\n",
    "\n",
    "4. **Check Shape of `y_train`**\n",
    "   - The comment `# y_train should have 1 column of credit score` is a reminder that `y_train` should contain only a single column representing credit scores, as it is a classification target for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n",
    "print(X_train_1.columns)\n",
    "print(y_train)\n",
    "# y_train should have 1 column of credit score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for XGBoost Classifier Using Optuna\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "   - `optuna`: For conducting hyperparameter optimization.\n",
    "   - `XGBClassifier` from `xgboost`: The machine learning model used for classification.\n",
    "   - `cross_val_score` from `sklearn.model_selection`: Used for performing cross-validation to evaluate model performance.\n",
    "\n",
    "2. **Define Objective Function for Optuna**\n",
    "   - The `objective` function defines the process Optuna will use to test and evaluate different parameter combinations.\n",
    "   - `params` dictionary specifies the search space for hyperparameters:\n",
    "     - `learning_rate`: A float between 0.01 and 0.2.\n",
    "     - `max_depth`: An integer between 3 and 10, controlling the depth of each tree.\n",
    "     - `n_estimators`: An integer between 100 and 500, determining the number of trees.\n",
    "     - `subsample`: A float between 0.5 and 1.0, which defines the fraction of samples used for training each tree.\n",
    "     - `colsample_bytree`: A float between 0.5 and 1.0, which specifies the fraction of features used by each tree.\n",
    "\n",
    "3. **Initialize the XGBoost Classifier with Trial Parameters**\n",
    "   - `model = XGBClassifier(**params, random_state=42, eval_metric=\"mlogloss\")`: Creates an instance of the XGBoost model with parameters generated by the current Optuna trial.\n",
    "\n",
    "4. **Cross-Validation to Evaluate Model**\n",
    "   - `accuracy = cross_val_score(model, X_train_1, y_train_1, cv=5, scoring=\"accuracy\").mean()` performs 5-fold cross-validation on the model, returning the mean accuracy score.\n",
    "\n",
    "5. **Run Bayesian Optimization with Optuna**\n",
    "   - `study = optuna.create_study(direction=\"maximize\")`: Initializes the optimization process with the goal of maximizing accuracy.\n",
    "   - `study.optimize(objective, n_trials=50)`: Runs the optimization for 50 trials.\n",
    "\n",
    "6. **Display Results**\n",
    "   - After optimization, print the best parameters and best accuracy:\n",
    "     - `print(\"Best parameters found: \", study.best_params)`\n",
    "     - `print(\"Best accuracy: \", study.best_value)`\n",
    "\n",
    "   The commented lines provide logs from sample trials showing trial results and the parameters that yielded the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    # Initialize the classifier with trial parameters\n",
    "    model = XGBClassifier(**params, random_state=42, eval_metric=\"mlogloss\")\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    accuracy = cross_val_score(model, X_train_1, y_train_1, cv=5, scoring=\"accuracy\").mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization with Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# [I 2024-11-03 16:53:08,619] Trial 0 finished with value: 0.7062031249999999 and parameters: {'learning_rate': 0.050893568127103876, 'max_depth': 4, 'n_estimators': 268, 'subsample': 0.6738283278380766, 'colsample_bytree': 0.6915738416826347}. Best is trial 0 with value: 0.7062031249999999.\n",
    "\n",
    "# [I 2024-11-03 16:58:00,159] Trial 2 finished with value: 0.7632656250000001 and parameters: {'learning_rate': 0.17031088174537234, 'max_depth': 9, 'n_estimators': 177, 'subsample': 0.8174475788946926, 'colsample_bytree': 0.5291010066051618}. Best is trial 2 with value: 0.7632656250000001.\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters found: \", study.best_params)\n",
    "print(\"Best accuracy: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training, Prediction, and Submission File Creation for Each Model in `models` Dictionary\n",
    "\n",
    "1. **Loop Through Models**\n",
    "   - `for key, value in models.items()`: Iterates through each model in the `models` dictionary, where `key` represents the model's name and `value` is the model instance.\n",
    "\n",
    "2. **Try-Except Block for Error Handling**\n",
    "   - Wraps the pipeline setup and predictions within a try-except block to catch and report any potential errors during the process.\n",
    "\n",
    "3. **Define Model Pipeline**\n",
    "   - `pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", value)])`: Creates a pipeline that combines data preprocessing (`preprocessor`) and the model (`classifier`), which allows both steps to be executed sequentially.\n",
    "\n",
    "4. **Fit the Pipeline on Training Data**\n",
    "   - `pipeline.fit(X_train, y_train)`: Trains the pipeline on the training data, applying the preprocessing steps followed by fitting the model.\n",
    "\n",
    "5. **Make Predictions on Test Data**\n",
    "   - `test_predictions = pipeline.predict(test_data)`: Generates predictions for the test dataset using the fitted pipeline.\n",
    "\n",
    "6. **Convert Predictions Back to Original Labels**\n",
    "   - `test_predictions_labels = label_encoder.inverse_transform(test_predictions)`: Converts encoded predictions back to the original label format using a `label_encoder`.\n",
    "\n",
    "7. **Prepare the Submission File**\n",
    "   - `submission = pd.DataFrame({\"ID\": test_ids, \"Credit_Score\": test_predictions_labels})`: Creates a DataFrame for submission with test IDs and predicted credit scores.\n",
    "   - `submission.to_csv(f\"submission_{key}.csv\", index=False)`: Saves the DataFrame as a CSV file with the model's name included in the filename.\n",
    "\n",
    "8. **Print Success Message**\n",
    "   - `print(f\"Submission file 'submission_{key}.csv' created successfully!\")`: Confirms successful file creation.\n",
    "\n",
    "9. **Handle Exceptions**\n",
    "   - In case of an error, prints an error message with details of the exception: `print(f\"Error : {e}\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in models.items():\n",
    "    try:  \n",
    "        # Define model pipeline\n",
    "        pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", value)])\n",
    "\n",
    "        # Fit the pipeline on training data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        test_predictions = pipeline.predict(test_data)\n",
    "\n",
    "        # Convert predictions back to original labels\n",
    "        test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "        # test_predictions_encoded = label_encoder.transform(test_predictions)\n",
    "        # Prepare the submission file\n",
    "        submission = pd.DataFrame(\n",
    "            {\"ID\": test_ids, \"Credit_Score\": test_predictions_labels}\n",
    "        )\n",
    "        submission.to_csv(f\"submission_{key}.csv\", index=False)\n",
    "\n",
    "        print(f\"Submission file 'submission_{key}.csv' created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
