{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_ids = test_data[\"ID\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data = train_data.drop(columns=[\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"Profession\", \"Number\", \"Loan_Type\"])\n",
    "\n",
    "# Fill missing values with median in train data\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric after removing underscores\n",
    "numeric_columns = [\n",
    "    \"Total_Current_Loans\", \"Current_Debt_Outstanding\", \"Income_Annual\", \n",
    "    \"Credit_Limit\", \"Age\", \"Total_Credit_Cards\", \"Total_Bank_Accounts\", \n",
    "    \"Delay_from_due_date\", \"Monthly_Balance\", \"Monthly_Investment\"\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    train_data[col] = pd.to_numeric(train_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "    test_data[col] = pd.to_numeric(test_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# Convert \"Credit_History_Age\" to numeric\n",
    "train_data[\"Credit_History_Age\"] = train_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "test_data[\"Credit_History_Age\"] = test_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "\n",
    "# Replace infinity and fill missing values in train data\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"Credit_Score\"] = label_encoder.fit_transform(train_data[\"Credit_Score\"])\n",
    "\n",
    "# Prepare training features and labels\n",
    "X_train = train_data.drop(columns=\"Credit_Score\")\n",
    "y_train = train_data[\"Credit_Score\"]\n",
    "\n",
    "# Replace problematic values and fill missing data for categorical features\n",
    "categorical_features = [\"Credit_Mix\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\", \"Total_Delayed_Payments\"]\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_train.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "test_data[categorical_features] = test_data[categorical_features].astype(str)\n",
    "test_data.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "\n",
    "# Define pipelines for preprocessing\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(test_data)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Define and train the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_predictions = logistic_model.predict(X_val_np)\n",
    "val_accuracy = accuracy_score(y_val_np, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = logistic_model.predict(X_test_preprocessed)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\"ID\": test_ids, \"Credit_Score\": test_predictions_labels})\n",
    "submission.to_csv(\"submission_LogisticRegression.csv\", index=False)\n",
    "print(\"Submission file 'submission_LogisticRegression.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_ids = test_data[\"ID\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data = train_data.drop(columns=[\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"Profession\", \"Number\", \"Loan_Type\"])\n",
    "\n",
    "# Fill missing values with median in train data\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric after removing underscores\n",
    "numeric_columns = [\n",
    "    \"Total_Current_Loans\", \"Current_Debt_Outstanding\", \"Income_Annual\", \n",
    "    \"Credit_Limit\", \"Age\", \"Total_Credit_Cards\", \"Total_Bank_Accounts\", \n",
    "    \"Delay_from_due_date\", \"Monthly_Balance\", \"Monthly_Investment\"\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    train_data[col] = pd.to_numeric(train_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "    test_data[col] = pd.to_numeric(test_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# Convert \"Credit_History_Age\" to numeric\n",
    "train_data[\"Credit_History_Age\"] = train_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "test_data[\"Credit_History_Age\"] = test_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "\n",
    "# Replace infinity and fill missing values in train data\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"Credit_Score\"] = label_encoder.fit_transform(train_data[\"Credit_Score\"])\n",
    "\n",
    "# Prepare training features and labels\n",
    "X_train = train_data.drop(columns=\"Credit_Score\")\n",
    "y_train = train_data[\"Credit_Score\"]\n",
    "\n",
    "# Replace problematic values and fill missing data for categorical features\n",
    "categorical_features = [\"Credit_Mix\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\", \"Total_Delayed_Payments\"]\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_train.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "test_data[categorical_features] = test_data[categorical_features].astype(str)\n",
    "test_data.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "\n",
    "# Define pipelines for preprocessing\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(test_data)\n",
    "\n",
    "# Convert preprocessed data to numpy arrays\n",
    "X_train_np = X_train_preprocessed\n",
    "y_train_np = np.array(y_train)\n",
    "X_test_np = X_test_preprocessed\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_train_np, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
    "\n",
    "# Define the SVM model\n",
    "svm_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_predictions = svm_model.predict(X_val_np)\n",
    "val_accuracy = accuracy_score(y_val_np, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = svm_model.predict(X_test_np)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\"ID\": test_ids, \"Credit_Score\": test_predictions_labels})\n",
    "submission.to_csv(\"submission_SVM.csv\", index=False)\n",
    "print(\"Submission file 'submission_SVM.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_ids = test_data[\"ID\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data = train_data.drop(columns=[\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"Profession\", \"Number\", \"Loan_Type\"])\n",
    "\n",
    "# Fill missing values with median in train data\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric after removing underscores\n",
    "numeric_columns = [\n",
    "    \"Total_Current_Loans\", \"Current_Debt_Outstanding\", \"Income_Annual\", \n",
    "    \"Credit_Limit\", \"Age\", \"Total_Credit_Cards\", \"Total_Bank_Accounts\", \n",
    "    \"Delay_from_due_date\", \"Monthly_Balance\", \"Monthly_Investment\"\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    train_data[col] = pd.to_numeric(train_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "    test_data[col] = pd.to_numeric(test_data[col].astype(str).str.replace(\"_\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# Convert \"Credit_History_Age\" to numeric\n",
    "train_data[\"Credit_History_Age\"] = train_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "test_data[\"Credit_History_Age\"] = test_data[\"Credit_History_Age\"].str.extract(\"(\\d+)\").astype(float)\n",
    "\n",
    "# Replace infinity and fill missing values in train data\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(train_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"Credit_Score\"] = label_encoder.fit_transform(train_data[\"Credit_Score\"])\n",
    "\n",
    "# Prepare training features and labels\n",
    "X_train = train_data.drop(columns=\"Credit_Score\")\n",
    "y_train = train_data[\"Credit_Score\"]\n",
    "\n",
    "# Replace problematic values and fill missing data for categorical features\n",
    "categorical_features = [\"Credit_Mix\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\", \"Total_Delayed_Payments\"]\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_train.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "test_data[categorical_features] = test_data[categorical_features].astype(str)\n",
    "test_data.replace([\"-\", \"NM\"], np.nan, inplace=True)\n",
    "\n",
    "# Define pipelines for preprocessing\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(test_data)\n",
    "\n",
    "# Convert preprocessed data to numpy arrays\n",
    "X_train_np = X_train_preprocessed.toarray()\n",
    "y_train_np = np.array(y_train)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_train_np, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
    "\n",
    "# Define the ANN model\n",
    "model = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_shape=(X_train_np.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation=\"softmax\")  # 3 classes for Credit_Score\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model.fit(X_train_np, y_train_np, validation_data=(X_val_np, y_val_np), epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_predictions = model.predict(X_val_np).argmax(axis=1)\n",
    "val_accuracy = accuracy_score(y_val_np, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(X_test_preprocessed.toarray()).argmax(axis=1)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\"ID\": test_ids, \"Credit_Score\": test_predictions_labels})\n",
    "submission.to_csv(\"submission_ANN.csv\", index=False)\n",
    "print(\"Submission file 'submission_ANN.csv' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
